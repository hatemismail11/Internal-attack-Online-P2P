\section{Introduction}
\label{sec:intro}



Over the last few years, the attraction, and thus reliance, on online video streaming P2P systems is continuously growing. 
As such systems are mainly characterized by being delay intolerant, churn prone and susceptible to attacks, robust data dissemination techniques that can accommodate for large scale applications had been exhaustively studied \cite{sasi2014survey}.
Among various existing architectures, mesh overlays mostly operate in a pull-based fashion \cite{zhang2014modeling}, where peers exchange available versus required chunks through a periodic buffer-map ($BM$) update.

Several attacks exist, and still evolving, that targets manipulating the $BM$ updates, known as $BM$ cheating attacks \cite{cheatingAnalysis}.
In a typical $BM$ cheating attack, malicious peers aim to lower the user's streaming experience via faking the content of their own $BM$s: 
(a) \textit{Drop}: by not including all the chunks available in their buffer, (b) \textit{Manipulation}: requesting chunks they already received, offering chunks they never received or delay sending actual chunks to other peers to out-date their buffer.
Accordingly, (i) honest peers become more overloaded trying to serve more chunk requests, and (ii) due to the streaming delay constraint, peers easily experience a low-quality and discontinuous stream.

Although various countermeasures exist \cite{zhang2005coolstreaming, defending2, antiliar}, those solutions render ineffective when cheating attacks is combined with inference attacks \cite{nguyen2016swap,4395124,rbcs}.
Inference attacks idea is the ability of the attacker to identify the closest peers to the source, i.e., peers directly connected to the source, we refer to those peers as headnodes.
Intuitively, attacking headnodes can drastically impact the overlay due to the criticality of such peers in dissemination video chunks received from the source to the rest of the overlay.

In \cite{nguyen2016swap, rbcs, nguyen2014resilience}, countermeasures to inference attacks are proposed. 
Nevertheless, the attacks proposed are external, i.e., the attacker is assumed to be able to shutdown headnodes only.
To the best of our knowledge, no one have considered internal inference attacks where the attacker is capable of placing malicious peers as headnodes.

In this work, we focus on the class of internal inference attacks conducting $BM$ cheating attacks, i.e., malicious headnodes conducting cheating attacks.
In fact, the existing countermeasures that address mitigating external inference attacks through diverse the set or periodically replacing the source's headnodes are inefficient against internal attacks.
The reason is that once the attacker control the source's headnodes, malicious peers can easily refuse to be swapped, fake reports to the source and launch $BM$ cheating which is harder to deal with as we detail in Section~\ref{sec:Attack}.

To that end, our contributions in this work are:
\begin{itemize}
 \item We show the feasibility of \textit{partially-fully} attacking the source's headnodes, which causes significant degradation in the user's satisfaction.
 \item Proposing a detection mechanism as a countermeasure against various adversarial behaviors deployed by malicious peers in an internal attack.
\end{itemize}

Our mechanism is two-fold. on the one hand, to counter dropping cheating behavior, peers cooperatively decide on filing a complaint to the source when upon agreeing on a certain dissatisfaction level.
Accordingly, the source is capable of detecting suspicious headnodes and thus, removes them to restore a stable data dissemination state.
On the other hand, we propose a local procedure (no global cooperation between peers) where each peer is able to detect manipulation from malicious peers and accordingly, remove suspicious peers from its own contact list.

Through a theoretical and simulation studies, we show that the proposed detection mechanism can fully restore users satisfaction/ experience, with minimal signaling overhead of $<10\%$, up to the range of 90-100\% in worst case scenarios where the attacker is fully occupying the source's headnodes.

% 
% at detection malicious behaviors in a fast, low overhead approach while considering the case where the attacker can fully occupy the source's neighbor list with malicious headnodes.
% In details, peers are able to trigger a request to their neighbors once (a) a peer is, with evidence, performing maliciously, or (b) generally, the stream satisfaction (the user's experience) of a peer drops below a given application threshold.
% Once proven malicious, a peer fires a complaint, on behalf of all other participants in the detection process, to the source to remove the detected peer from it's neighbor list,
% or to generally decide about peers that might be affecting the satisfaction level of non-headnodes.
\subsection*{Paper Structure}
The rest of the paper is organized as follows: Section~\ref{sec:related} discusses the related work and the background.
In Section~\ref{sec:Attack}, the concepts underlying the attacker model internal attack and the conducted adversarial behaviors are defined.
The detection mechanism is fully discussed in Section~\ref{sec:detection}, with the theoretical analysis is discussed in Section~\ref{sec:analysis}.
The internal attack's impact, along with the detection mechanism performance and efficiency, are evaluated in Section~\ref{sec:eval}. Finally, we conclude our work and discuss the future work in Section~\ref{sec:conclusion}.





 


