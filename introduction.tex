\section{Introduction}
\label{sec:intro}

Streaming content is an essential aspect of today's data driven infrastructure \cite{emule}.
Most streaming applications rely on central providers that have significant resources at their hands and hence distribute content to a large audience in a fast and reliable manner. 
However, these enormous costs of guaranteeing high quality-of-service when streaming impede alternative providers and content, entailing oligopolies.
Controlling or influencing these few highly visible providers might allow censorship or propaganda, giving powerful parties the opportunity  to dictate the public opinion and undermine freedom of speech.
% \mn[Stef]{feel free to remove the censorship/propaganda aspect}   
Distributing content via a Peer-to-Peer (P2P) network significantly lowers the load that the provider experiences, as participants relay the downloaded content to other participants, eliminating the need that all participants receive their content directly from the source, i.e., the provider. 
Consequentially, P2P streaming is an attractive option for alternative providers and user-generated content.
Furthermore, including P2P streaming into the service of established providers will reduce their costs and hence should lead to lower rates for customers or decrease the amount of advertisement within the system, depending on the providers' business model. 


However, achieving reliability in the face of the dynamic heterogeneous group of content distributors in a P2P overlay is challenging. In addition to the inherent unreliability of benign participants, attackers such as competitors can infiltrate the set of peers and perform denial-of-service attacks, which interrupt or delay the distribution of the content with the goal of degrading the quality of service. 

In general, nodes in a P2P streaming system connect to a small set of other nodes, called the \emph{neighbor set}. 
The publisher or the \emph{source} of the content divides the stream into \emph{chunks}, each contains a part of the encoded data, and forwards these chunks to its neighbors. Nodes in the system receive chunks from neighbors and forward them to neighbors that have not previously received the chunks. 
The selection of neighbors and the choice of which neighbors to receive from or forward to differ between protocols \cite{sasi2014survey}.  Yet, pull-based mesh networks are the predominant method in P2P streaming systems \cite{zhang2014modeling}. In a mesh overlay, peers maintain a buffer-map indicating which chunks they possess.  Neighbors periodically exchange their buffer-maps and request chunks from neighbors whose buffer-maps indicate possession of the respective chunk. Peers then forward chunks based on the received requests. 


In pull-based systems, there exist three types of denial-of-service attacks, known as buffer-map or $BM$ cheating attacks \cite{cheatingAnalysis}. 
In such an attack, a node might drop or delay chunks. Alternatively, they might advertise chunks that they do not have. As detection of the latter is locally possible and the effect of delaying chunks is at most as severe as entirely dropping the respective chunks, we focus on a denial-of-service attack through dropping chunks without advertising them, called \drop in the following.  


In the past, multiple countermeasures aimed to reduce the severity of\drop attack. However, the majority of these defences \cite{zhang2005coolstreaming, defending2, antiliar} assumes that the attacker is unaware of the topology of the streaming network and specifically does not know the \emph{headnodes}, i.e., the peers connected to the source. 
However, previous work indicates that it is relatively easy to infer the identity of benign headnodes and then target those important nodes \cite{nguyen2016swap}.
While countermeasures to these inference attacks exist, they assume an external adversary that can shut-down or replace certain nodes \cite{nguyen2016swap, rbcs, nguyen2014resilience}. 
Thus, the existing work neither evaluates the impact of internal colluding attackers, i.e., attackers that insert nodes under their control into the system pretending to be regular participants, nor proposes a protection mechanism against these attacks.


In this paper, we  illustrate the effectiveness of internal attacks that benefit from malicious headnodes. 
Consequentially, we propose a mechanism for detecting \drop. Our protection mechanism keeps track of peers' satisfaction. 
If the cumulative satisfaction level of a group of peers drops below a certain threshold, the source replaces all headnodes associated with the group with randomly chosen peers. 
Our detection mechanism hence reacts to a low quality-of-service rather than explicitly identifying the misbehavior of one or more specific nodes.
Note that we focus on attackers that control a low fraction of nodes, as attackers controlling the majority of nodes can trivially control most of the communication, even without gaining strategically important positions such as headnodes.    

In an extensive simulation study, we show that the proposed detection mechanism can fully restore peers' satisfaction, even in a scenario where the attacker controls all headnodes. 
Our mechanism only introduces a signaling overhead of less than $8\%$, meaning that it is both effective and efficient.

A theoretical analysis complements our concrete practical results, focusing on the opportunities to abuse the detection mechanism. 
Indeed, the detection algorithm mostly prevents malicious nodes from replacing benign headnodes with malicious nodes unless they control a large fraction of the total number of nodes. 
Furthermore, maintaining malicious headnodes despite generally low satisfaction levels is not possible for the considered attacker. 
  



% 
% at detection malicious behaviors in a fast, low overhead approach while considering the case where the attacker can fully occupy the source's neighbor list with malicious headnodes.
% In details, peers are able to trigger a request to their neighbors once (a) a peer is, with evidence, performing maliciously, or (b) generally, the stream satisfaction (the user's experience) of a peer drops below a given application threshold.
% Once proven malicious, a peer fires a complaint, on behalf of all other participants in the detection process, to the source to remove the detected peer from it's neighbor list,
% or to generally decide about peers that might be affecting the satisfaction level of non-headnodes.
% \subsection*{Paper Structure}
% \mn[Stef]{I dislike paper structure sections, but that is personal taste; I can deal with them by ignoring them, which is what I did with this one}
% The rest of the paper is organized as follows: Section~\ref{sec:related} discusses the related work and the background.
% In Section~\ref{sec:Attack}, the concepts underlying the attacker model internal attack and the conducted adversarial behaviors are defined.
% The detection mechanism is fully discussed in Section~\ref{sec:detection}, with the theoretical analysis is discussed in Section~\ref{sec:analysis}.
% The internal attack's impact, along with the detection mechanism performance and efficiency, are evaluated in Section~\ref{sec:eval}. Finally, we conclude our work and discuss the future work in Section~\ref{sec:conclusion}.





 


