\section{Detection Mechanism}
\label{sec:detection}


In this section, we explain our detection mechanism, starting with an overview of the different steps followed by a detailed description of each step.  
The goal of the detection algorithm is to restore the user's satisfaction in the face of a \drop attack. The key idea of our method is to replace headnodes associated with peer groups of low satisfaction levels. 

Throughout the section, we assume that nodes authenticate their messages using digital signatures. 
% Hatem: trim
% The source keeps track of the verification keys of all participants and can hence establish the authenticity of messages.
The source keeps track of participants' verification keys and can hence establish the authenticity of messages.
In particular, malicious nodes cannot forge responses of honest peers to influence the mechanism. 
We assume that neighboring nodes periodically exchange messages stating that they are neighbors.
These \emph{proofs of neighborhood} are signed and contain a time stamp. 
In this manner, $u$ can proof if $v$ is (or has recently been) its neighbor. 





\subsection{Mechanism Overview}
We illustrate the underlying ideas of the detection mechanism in Figure~\ref{detection-blocks}.
When a malicious peer $m$ performs a \drop attack, benign peers $b$ are unable to immediately identify the malicious behavior.
Specifically, $m$ never sends the actual $BM$ that represents the chunks it currently possesses, i.e., $m$ is only requesting chunks it already has. 
Thus, detecting a violation in this case is not straight-forward. In particular, nodes are generally unable to identify a suspected attacker based only on local information.
In the remainder of the section, we present a mechanism that allows nodes to collaboratively identify suspects that are subsequently removed as headnodes.  




\begin{figure}
 \centering
 \includegraphics[width=5.5cm,height=6.2cm]{./Figures/detection.pdf}
 
  \caption{detection process for \drop. $S$ denotes the source.}
   \vspace{-3.5mm}
\label{detection-blocks} 
\end{figure}



The detection consists of four steps, starting with an initial trigger of dissatisfaction at one peer and potentially terminating in replacing one or several headnodes. 
First, when a peer $b$ suspects a \drop attack based on its local observations, $b$ sends a \emph{detection request} to all peers in its neighbor list.
Second, each peer receiving a detection request prepares a response. 
Third, the initiator $b$ decides based on the received responses if they should file a complaint to the source. 
If $b$ decides to file the complaint, $b$ sends it on behalf of the participating peers in the request. 
Afterwards, the source verifies the complaint, reacts accordingly, and responds to $b$, detailing the steps taken. 
The reaction of the source is either the replacement of one or several headnodes or the rejection of the removal request.
Finally, $b$ reacts based on the received response from the source and then forwards the source's reply to the other participants in the complaint, who in turn execute the same procedure.

The node $b$ bases its decision on whether to initiate a request or forward a complaint on a number of threshold parameters, which we summarize in Table~\ref{tab:acronyms} together with various system parameters governing the attack. 


\begin{table}
\center

\caption{ACRONYMS}
\begin{tabular}{|c|l||c|l|}
\hline

\bf{Var.} & \bf{Description}  & \bf{Var.} & \bf{Description} \\\hline\hline
$H_n$ & headnodes in a complaint & $P_n$ & potential candidates \\\hline
$x$ & no. of malicious peers & $x_h$ & mal. headnodes \\\hline
$BM$ & buffer-map & $x-x_h$ & mal. non-headnodes \\\hline
$\sat$ & peer satisfaction level & $\satThres$ & satisfaction threshold \\\hline
$\minP$ & min. drop responses & $\minDR$ & no. allowed det. \\\hline
\end{tabular}
\label{tab:acronyms}
\vspace{-2.5mm}
\end{table}

\subsection{Detection Trigger}
\label{Detection-Trigger}


In order to start a detection request, the node $b$ has to experience a low satisfaction level. 
The satisfaction of a peer is defined as the fraction of missed chunks, i.e., the continuity of the stream according to the $Hit/Hit+miss$ chunk ratio.
% \mn[Stef]{how many chunks does the peer consider for that; all during the whole download, only the last $s$ seconds?}
In a nutshell, $b$ starts a detection request if its satisfaction is below a satisfaction threshold $\satThres$.
However, to limit the ability of malicious peers to incorrectly accuse benign peers and increase the load through false detection requests, the concrete conditions that result in a detection request from $b$ are: 

% \begin{itemize}[noitemsep,wide=0pt, leftmargin=\dimexpr\labelwidth + 2\labelsep\relax]

\begin{enumerate}
% \begin{compactenum}
 \item $b$'s current satisfaction level $\sat_b$ is less than the predefined threshold, i.e., $\sat_b < \satThres$.
 \item The number of drop detection requests sent by $b$ in the time interval $t_{det}$ is less than $\kappa$. 
 \item $b$ has not initiated or replied to any other \drop detection request that the source has not decided on yet.
% \end{compactenum}
\end{enumerate}
The second and third condition guarantee that peers cannot abuse the mechanism via triggering or participating in multiple detection requests in parallel. 
Moreover, restricting concurrent requests for benign peers is sensible as their low satisfaction level is already noted in their reply to previous requests.

\subsection{Processing a Detection Request}
Let $D$ denote the set of queried peers, i.e., the neighbors of the initiator $b$ if $b$ executes the protocol honestly.  
When receiving a detection request, a peer $d \in D$ hence first checks if it can participate in any more requests.     
If so, $d$ replies with its $\sat_d$ and a time stamp, both signed by its private signature key. 
The time stamp prevents the attacker from replaying benign peers' previous (low) satisfaction levels, as only recent satisfaction levels are valid.


\subsection{Filing and Processing a Complaint}

Upon receiving a feedback from its neighbors, $b$ decides whether to file a complaint or not.
If so, the source verifies the complaint and potentially removes some of its headnodes. 

\subsubsection*{Filing a complaint}
$b$ will start processing the replies once all nodes in $D$ have replied or a time-out $t_{replies}$ occurs. 
We assume that the source's address is publicly known and $b$ can send a complaint to the source directly.

$b$ sends a complaint if the average satisfaction level indicated in the responses is below a threshold $\satThres$ and at least $\minP$ peers replied to the request. 
More specifically, let $\sat_1, \ldots ,\sat_z$ be the satisfaction levels expressed in the replies and $\sat_b$ be $b$'s satisfaction level. 
Assuming a sufficient number of replies, $b$ files a complaint to the source if:
\begin{align}
\label{eq:drop_satis_equation}
\frac{1}{z+1}\left( \sat_b + \sum_{i=1}^{z} \sat_i \right) < \satThres. 
\end{align} 
Otherwise, $b$ either starts another detection request depending on $\kappa$ or waits until allowed to send another detection request.
 

Once $b$ decides on filing a complaint according to the aforementioned conditions, $b$ generates a complaint message to the source containing the IDs of all nodes in $D$, recent proofs of neighborhood,  and the received satisfaction levels including signatures and time stamps.  


The reason for requesting at least $\minP$ replies is to prevent a few malicious nodes from accusing benign headnodes. By imposing a lower bound on the requested number of replies, a considerable number of malicious nodes has to use one of their $\kappa$ requests. We present an in-depth analysis on how these constraints prevent misuse in Section~\ref{sec:analysis}. 

\subsubsection*{Processing a Complaint at the Source}

The source $s$ first verifies the content of the complaint. First, the source rejects any complaint from a node $b$ that has already participated in $\kappa$ requests. 
If $s$ does not reject the complaint, $s$ then removes any satisfaction levels without valid signatures from the complaint.
Furthermore, $s$ removes any responses from nodes that have exceeded their participation limit or are participating in two complaints at the same time. 

If the remaining valid responses still indicate an average satisfaction level of less than $\satThres$, the source:  
\begin{enumerate}
 \item divides the set of peers in $D$ into two sets $H_n$ and $P_n$, where $H_n$ is the set of headnodes peers that exist in the complaint. 
%  \mn[Stef]{Itemize lists are strange as they are not full sentences but start with a capital letter and end in .}
 \item removes all peers in $H_n$ from its neighbor set.
 \item randomly connects to another $|H_n|$ peers. 
 \item adds peers (excluding peers in $H_n$) from its neighbor list to $P_n$, where $P_n = NeighborList\setminus H_n$ ($NeighborList$ is the set of peers in a neighbor list). 
 \item sends a \textit{Complaint Reply} to $b$ containing $H_n$ and $P_n$.
\end{enumerate}
 The reason for choosing random new headnodes rather than nodes participating in the complaint is to lower the incentive for complaints by malicious peers.
 Even if such a complaint is successful, the new headnodes are likely benign, meaning that the malicious nodes did not gain anything from initiating the request apart from slightly increasing the load. 


\subsubsection*{Processing a Complaint Reply \& Forwarding}

Finally, when $b$ receives the \textit{complaint Reply} from the source, $b$ 
\begin{enumerate}
 \item Disconnects from all peers in $H_n$. Note that $b$ does not blacklist peers in $H_n$ from its neighbor list due to the fact that those peers are not proven malicious.
 \item Connects to $|H_n|$ peers from $P_n$, in case $|H_n|>|P_n|$, peers connect to $|P_n|+(|H_n|-|P_n|)$ random peers.
\end{enumerate}
Subsequently, $b$ forwards the complaint to the other participants, i.e., $D \setminus H_n$, who in turn execute steps 1 and 2.

\subsection{General Notes}
The detection mechanism does not aim at expelling peers from the system. Simply removing headnodes remarkably benefit the system.
Indeed, the only peers that get blacklisted are those who violate the detection mechanism constraints, i.e., participating in more than a single request at a time or initiating more than $\kappa$ requests.
The reason for this leniency lies in the potentially high chance of removing headnodes that are benign but exhibit a low performance. 
In general, the main target of the detection mechanism is to enhance peers' satisfaction level while keeping peer replacements and signaling overhead low. 






